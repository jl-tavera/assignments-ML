{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerelu(Z):\n",
    "    \"\"\"\n",
    "    Implementar la función de forward usando Leaky RELU\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Para este paso vamos a categorizar los valores observados en Z en positivos (Z_pos)\n",
    "    y negativos (Z_neg)\n",
    "    \"\"\"\n",
    "    Z_pos = np.maximum(0,Z)\n",
    "    Z_neg = np.minimum(0,Z)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Se definirá alfa arbitrariamente como 0.02\n",
    "    \"\"\"\n",
    "    A = Z_pos + (0.01) * Z_neg\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerelu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implementar el proceso de backpropagation para la función Leaky RELU\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dZ = gradiente del costo con respecto a la salida lineal Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # se usa para asegurarse que el gradente dZ pueda ser trabjado más adelante\n",
    "    \n",
    "    # En este caso, la derivada para Z<=0 es única y es igual a 0.01, por lo que se mantiene ese número como el gradiente para todo Z por debajo de 0. \n",
    "    dZ[Z <= 0] = 0.01\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tanh(Z):\n",
    "    \"\"\"\n",
    "    Implementar la función de forward para una función de activación Tanh\n",
    "    Z es el conjunto de datos input\n",
    "    A es el resultado de la implementación  \n",
    "    \"\"\"\n",
    "    \n",
    "    A = (np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tanh_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implementar el proceso de Backpropagation para una unidad de \n",
    "    Tanh\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = dA * (1-np.square((np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))))\n",
    "\n",
    "    assert(dZ.shape == Z.shape)\n",
    "\n",
    "    return(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Argumentos:\n",
    "    A -- datos de input (tamaño de la capa anterior, número de ejemplos)\n",
    "    W -- matriz de pesos: arreglo de forma en numpy  (tamaño de la capa actual, 1)\n",
    "    b -- vector sezgo, arreglo de forma en numpy (tamaño de la capa actual, 1)\n",
    "\n",
    "    Retornos:\n",
    "    Z -- el input de la funcion de activación, también conocido como parámetro de activación \n",
    "    cache -- un diccionario de de python que contiene: \"A\", \"W\" and \"b\" ; se guardan para facilitar el cálculo del backward más adelante\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementar la propagación linela hacia atrás de una sola iteración lineal (layer l)\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- Gradiente del costo con respecto al output lineal (de la capa actual l)\n",
    "    cache -- lista de valores (A_prev, W, b)que vienen de la propagación forward de la capa actual\n",
    "\n",
    "    Retornos:\n",
    "    dA_prev -- Gradiente del costo con respecto a la activación (de la capa anterior l-1), con la misma forma que A_prev\n",
    "    dW -- Gradiente del costo con respecto a W (capa actual l), misma forma que W\n",
    "    db -- Gradiente del costo con respecto a b (capa actual l), misma forma que b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
